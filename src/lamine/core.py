"""Contains main functions exposed to the user to interact with LLM APIs."""

import logging
import random
from importlib import import_module
from time import time

from .datatypes import Answer, Message, Model, _Model
from .utils import parallelize_function, return_if_exception


def get_answer(model: Model, conversation: list[Message], **kwargs) -> Answer:  # type: ignore
    """
    Requests a response from a language model API for a single conversation.

    Args:
        model (Model): The language model to use for generating the response.
        conversation (list[Message]): A list of messages representing the conversation history.
        **kwargs: Additional keyword arguments like `temperature` or `top_p`.

    Returns:
        Answer: The response generated by the language model.
    """
    # import provider
    module = import_module(f"lamine.providers.{model.provider}")
    provider = getattr(module, f"{model.provider.capitalize()}")()

    # make queryable _Model(s)
    if model.locations:
        models = [_Model(model.provider, model.id, loc) for loc in model.locations]
        random.shuffle(models)
    else:
        models = [_Model(model.provider, model.id)]

    # run queries in fallback cycle
    for i, m in enumerate(models):
        try:
            start = time()
            answer = provider.get_answer(model=m, conversation=conversation, **kwargs)
            answer.time = round(time() - start, 3)
            logging.info(f"Request to '{m.to_str}' succeeded")
            return answer
        except Exception as e:
            logging.error(f"Request to '{m.to_str}' failed: '{e}'")
            if i == len(models) - 1 and len(models) > 1:  # last model
                logging.error("All locations failed to respond.")
                raise


@return_if_exception
def _get_answer(*args, **kwargs):
    return get_answer(*args, **kwargs)


def get_answers(model: Model, conversations: list[list[Message]], **kwargs) -> list[Answer | Exception]:
    """
    Request responses from a language model API for multiple conversations in parallel.

    Args:
        model (Model): The language model to use for generating the responses.
        conversations (list[list[Message]]): A list of conversations, where each conversation is a list of messages.
        **kwargs: Additional keyword arguments like `temperature` or `top_p`.

    Returns:
        list[Answer]: A list of responses generated by the language model for each conversation.
    """
    params_list = []
    for conversation in conversations:
        params_list.append({"model": model, "conversation": conversation, "**kwargs": kwargs})
    return parallelize_function(function=_get_answer, params_list=params_list)
