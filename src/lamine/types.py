"""Contains all types defined to represent interactions with LLM APIs. Utility parent types are defined in .utils"""

import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass
from importlib import import_module
from typing import Optional

from .providers import PROVIDERS
from .utils import BaseDataclass, BaseEnum


@dataclass
class Message(BaseDataclass):
    """
    Represents a message in a conversation.

    Attributes:
        role (str): The role of the message sender (e.g., "user", "assistant").
        content (str): The content of the message.
    """

    role: str
    content: str


@dataclass
class Answer(BaseDataclass):
    """
    Represents an answer generated by a language model.

    Attributes:
        content (str): The content of the answer.
        tokens_in (int): The number of input tokens used to generate the answer.
        tokens_out (int): The number of output tokens generated in the answer.
        time (float): The time taken to generate the answer in seconds.
    """

    content: str
    tokens_in: int
    tokens_out: int
    time: Optional[float] = None

    @property
    def message(self) -> Message:
        """
        Converts the answer to a Message object with the role "assistant".

        Returns:
            Message: A Message object representing the answer.
        """
        return Message("assistant", self.content)


@dataclass
class Action(BaseDataclass):
    """
    Represents an tool request by a Language Model.

    Attributes:
        name (str): The name of the tool.
        params (dict): A dictionary with input parameters.
    """

    name: str
    params: dict


@dataclass
class Observation(BaseDataclass):
    """
    Represents the result of an Action.

    Attributes:
        status (str): The status of the observation (e.g., "success", "failure").
        content (str): The content of the observation.
    """

    status: str
    content: str


@dataclass
class Model(BaseDataclass):
    """
    Represents a language model.

    Attributes:
        provider (str): The provider of the language model (e.g., "OpenAI", "Anthropic").
        id (str): The unique identifier of the language model.
        locations (list[str]): A list of host locations to randomly pick from.
    """

    provider: str
    id: str
    locations: Optional[list[str]] = None

    def __post_init__(self):
        """Checks whether the combination of provider, id and locations is valid."""
        # check if provider is supported
        self.provider = self.provider.lower()
        if self.provider not in PROVIDERS:
            raise ValueError(f"Provider {self.provider} is not supported: {PROVIDERS.values}.")

        # check if model_id is supported
        module = import_module(f"lamine.providers.{self.provider}")
        provider = getattr(module, f"{self.provider.capitalize()}")()
        if self.id not in provider.model_ids:
            logging.warning(f"Provider {self.provider} does not support model {self.id}: {provider.model_ids.values}")

        # check if locations are supported
        if self.locations:
            module = import_module(f"lamine.providers.{self.provider}")
            if not provider.locations:
                logging.warning(f"Provider {self.provider} does not support `locations`.")
            else:
                for location in self.locations:
                    if location not in provider.locations:
                        logging.warning(
                            f"Provider {self.provider} does not support location {location}: {provider.locations.values}"
                        )


class Provider(ABC):
    """
    Abstract class defining the shared interface for API providers.

    Attributes:
        model_ids (list[str]): a list of valid model ids supported by the provider, e.g. "gemini-2-flash".
        locations (list[str]): a list of valid locations where the provider is hosts LMs, e.g. "eu-central1".
            This generally applies only to provideders Vertex, Bedrock and Azure.
    """

    model_ids: list[str]
    locations: Optional[list[str]] = None

    def __init__(self):
        self.model_ids = BaseEnum.from_list("model_ids", self.model_ids) if self.model_ids else None
        self.locations = BaseEnum.from_list("locations", self.locations) if self.locations else None

    @abstractmethod
    def get_answer(self, model: Model, conversation: list[Message], **kwargs) -> Answer:
        """
        Requests a response from a language model API for a single conversation.

        Args:
            model (Model): The language model to use for generating the response.
            conversation (list[Message]): A list of messages representing the conversation history.
            **kwargs: Additional keyword arguments like `temperature` or `top_p`.

        Returns:
            Answer: The response generated by the language model.
        """
        ...
