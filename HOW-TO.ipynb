{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "LaMINe makes it as easy as possible to call the a LM API and get an answer.\n",
                "\n",
                "## Minimum example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "\"Here's a simple Spaghetti Aglio e Olio (Garlic and Oil) recipe:\\n\\nIngredients:\\n- 1 lb spaghetti\\n- 1/2 cup olive oil\\n- 6 garlic cloves, thinly sliced\\n- Red pepper flakes\\n- Chopped parsley\\n- Salt and pepper\\n\\nInstructions:\\n1. Boil pasta in salted water\\n2. While pasta cooks, sautÃ© garlic in olive oil until golden\\n3. Add red pepper flakes\\n4. Drain pasta, reserve some water\\n5. Toss pasta in garlic oil\\n6. Add parsley, salt, and pepper\\n7. Add pasta water if needed for consistency\\n\\nEnjoy!\""
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from lamine.core import get_answer\n",
                "from lamine.types import Message, Model\n",
                "\n",
                "model = Model(\"anthropic\", \"claude-3-5-haiku-latest\")\n",
                "conversation = [Message(\"user\", \"Gimme a brief pasta recipe\")]\n",
                "answer = get_answer(model, conversation)\n",
                "answer.content"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Auto-checks\n",
                "When initializing a `Model`: \n",
                "- warnings will be raised if the combination of `provider` and `model_id` does not exist\n",
                "- an error will be raised if you are missing the environmental variables to query the provider."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING:root:Provider 'anthropic' does not support model 'claudia-5-haiku-latest': ['claude-3-5-sonnet-latest', 'claude-3-5-haiku-latest', 'claude-3-haiku-20240307']\n"
                    ]
                },
                {
                    "ename": "ValueError",
                    "evalue": "Provider 'ankropic' is not supported: ['together', 'anthropic', 'vertex'].",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaudia-5-haiku-latest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mankropic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclaude-3-5-haiku-latest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32m<string>:6\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, provider, id, locations)\u001b[0m\n",
                        "File \u001b[1;32mF:\\lamine\\src\\lamine\\types.py:123\u001b[0m, in \u001b[0;36mModel.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m PROVIDER_IDS:\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvider \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not supported: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROVIDER_IDS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# load provider for further checks\u001b[39;00m\n\u001b[0;32m    126\u001b[0m module \u001b[38;5;241m=\u001b[39m import_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlamine.providers.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
                        "\u001b[1;31mValueError\u001b[0m: Provider 'ankropic' is not supported: ['together', 'anthropic', 'vertex']."
                    ]
                }
            ],
            "source": [
                "model = Model(\"anthropic\", \"claudia-5-haiku-latest\")\n",
                "model = Model(\"ankropic\", \"claude-3-5-haiku-latest\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Locations\n",
                "- Some providers hosts models in different locations\n",
                "- You can select your allowed ones when initializing a model\n",
                "- If the location is not valid, a warn will be given\n",
                "- If multiple locations are selected, for each request one location will be picked at random."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "WARNING:root:Provider 'vertex' does not support location 'eu-central1': ['europe-west6', 'europe-west2', 'europe-west8', 'europe-north1', 'europe-central2', 'australia-southeast1', 'us-west4', 'us-east5', 'asia-south1', 'us-west3', 'europe-west9', 'me-central2', 'us-west1', 'asia-northeast3', 'africa-south1', 'me-west1', 'asia-northeast1', 'europe-west3', 'europe-west12', 'asia-southeast1', 'us-east1', 'asia-southeast2', 'europe-west1', 'us-west2', 'me-central1', 'northamerica-northeast1', 'europe-west4', 'southamerica-west1', 'us-east4', 'southamerica-east1', 'northamerica-northeast2', 'europe-southwest1', 'us-south1', 'asia-northeast2', 'australia-southeast2', 'asia-east1', 'us-central1', 'asia-east2']\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Querying 'vertex' server in location 'europe-central2'\n",
                        "Querying 'vertex' server in location 'europe-central2'\n",
                        "Querying 'vertex' server in location 'europe-central2'\n",
                        "Querying 'vertex' server in location 'europe-west6'\n",
                        "Querying 'vertex' server in location 'europe-west6'\n"
                    ]
                }
            ],
            "source": [
                "model = Model(\"vertex\", \"gemini-1.5-flash-002\", [\"eu-central1\"])\n",
                "model = Model(\"vertex\", \"gemini-1.5-flash-002\", [\"europe-central2\", \"europe-west6\"])\n",
                "for _ in range(5):\n",
                "    get_answer(model, conversation)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Batch requests\n",
                "You can request multiple requests to be solved in parallel."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'Model' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlamine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_answers\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtogether\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3-8b-chat-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m conversations \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      5\u001b[0m     [Message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGimme a brief pasta recipe\u001b[39m\u001b[38;5;124m\"\u001b[39m)],\n\u001b[0;32m      6\u001b[0m     [Message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGimme a brief salad recipe\u001b[39m\u001b[38;5;124m\"\u001b[39m)],\n\u001b[0;32m      7\u001b[0m ]\n\u001b[0;32m      8\u001b[0m answers \u001b[38;5;241m=\u001b[39m get_answers(model, conversations)\n",
                        "\u001b[1;31mNameError\u001b[0m: name 'Model' is not defined"
                    ]
                }
            ],
            "source": [
                "from lamine.core import get_answers\n",
                "from lamine.types import Message, Model\n",
                "\n",
                "model = Model(\"together\", \"meta-llama/Llama-3-8b-chat-hf\")\n",
                "conversations = [\n",
                "    [Message(\"user\", \"Gimme a brief pasta recipe\")],\n",
                "    [Message(\"user\", \"Gimme a brief salad recipe\")],\n",
                "]\n",
                "answers = get_answers(model, conversations)\n",
                "print([answer.content for answer in answers])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
